{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to find the largest contour\n",
    "def get_largest_contour(contours):\n",
    "    max_contour = max(contours, key=cv2.contourArea)\n",
    "    return max_contour\n",
    "\n",
    "# Function to detect hand gesture\n",
    "def detect_gesture(hand_contour):\n",
    "    # Convex hull and convexity defects\n",
    "    hull = cv2.convexHull(hand_contour, returnPoints=False)\n",
    "    defects = cv2.convexityDefects(hand_contour, hull)\n",
    "    \n",
    "    if defects is None:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    count_defects = 0\n",
    "\n",
    "    for i in range(defects.shape[0]):\n",
    "        s, e, f, d = defects[i, 0]\n",
    "        start = tuple(hand_contour[s][0])\n",
    "        end = tuple(hand_contour[e][0])\n",
    "        far = tuple(hand_contour[f][0])\n",
    "\n",
    "        # Calculate the length of all sides of the triangle\n",
    "        a = np.linalg.norm(np.array(start) - np.array(end))\n",
    "        b = np.linalg.norm(np.array(far) - np.array(start))\n",
    "        c = np.linalg.norm(np.array(end) - np.array(far))\n",
    "\n",
    "        # Calculate the area using Heron's formula\n",
    "        s = (a + b + c) / 2\n",
    "        area = np.sqrt(s * (s - a) * (s - b) * (s - c))\n",
    "\n",
    "        # Distance between point and convex hull\n",
    "        d = (2 * area) / a\n",
    "\n",
    "        # Angle between start and end\n",
    "        angle = np.arccos((b**2 + c**2 - a**2) / (2 * b * c)) * (180 / np.pi)\n",
    "\n",
    "        # Ignore small angles to filter out noise\n",
    "        if angle <= 90 and d > 30:\n",
    "            count_defects += 1\n",
    "\n",
    "    if count_defects == 0:\n",
    "        return \"Rock\"\n",
    "    elif count_defects == 1:\n",
    "        return \"Scissors\"\n",
    "    elif count_defects >= 4:\n",
    "        return \"Paper\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Flip the frame so it acts as a mirror\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Define ROI\n",
    "        roi = frame[100:400, 100:400]\n",
    "        \n",
    "        # Preprocessing the ROI\n",
    "        gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        blur = cv2.GaussianBlur(gray, (35, 35), 0)\n",
    "        _, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "        \n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if len(contours) > 0:\n",
    "            hand_contour = get_largest_contour(contours)\n",
    "            gesture = detect_gesture(hand_contour)\n",
    "            \n",
    "            # Draw ROI and contour\n",
    "            cv2.rectangle(frame, (100, 100), (1000, 1000), (0, 255, 0), 2)\n",
    "            cv2.drawContours(roi, [hand_contour], -1, (0, 0, 255), 3)\n",
    "            \n",
    "            # Display the gesture\n",
    "            cv2.putText(frame, gesture, (100, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow(\"Gesture Detection\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe\n",
    "import cv2\n",
    "\n",
    "#Use MediaPipe to draw the hand framework over the top of hands it identifies in Real-Time\n",
    "drawingModule = mediapipe.solutions.drawing_utils\n",
    "handsModule = mediapipe.solutions.hands\n",
    "\n",
    "#Use CV2 Functionality to create a Video stream and add some values\n",
    "cap = cv2.VideoCapture(0)\n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "\n",
    "#Add confidence values and extra settings to MediaPipe hand tracking. As we are using a live video stream this is not a static\n",
    "#image mode, confidence values in regards to overall detection and tracking and we will only let two hands be tracked at the same time\n",
    "#More hands can be tracked at the same time if desired but will slow down the system\n",
    "with handsModule.Hands(static_image_mode=False, min_detection_confidence=0.7, min_tracking_confidence=0.7, max_num_hands=2) as hands:\n",
    "\n",
    "#Create an infinite loop which will produce the live feed to our desktop and that will search for hands\n",
    "     while True:\n",
    "           ret, frame = cap.read()\n",
    "           #Unedit the below line if your live feed is produced upsidedown\n",
    "           #flipped = cv2.flip(frame, flipCode = -1)\n",
    "           \n",
    "           #Determines the frame size, 640 x 480 offers a nice balance between speed and accurate identification\n",
    "           frame1 = cv2.resize(frame, (640, 480))\n",
    "           \n",
    "           #Produces the hand framework overlay ontop of the hand, you can choose the colour here too)\n",
    "           results = hands.process(cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB))\n",
    "           \n",
    "           #In case the system sees multiple hands this if statment deals with that and produces another hand overlay\n",
    "           if results.multi_hand_landmarks != None:\n",
    "              for handLandmarks in results.multi_hand_landmarks:\n",
    "                  drawingModule.draw_landmarks(frame1, handLandmarks, handsModule.HAND_CONNECTIONS)\n",
    "                  \n",
    "                  #Below is Added Code to find and print to the shell the Location X-Y coordinates of Index Finger, Uncomment if desired\n",
    "                  #for point in handsModule.HandLandmark:\n",
    "                      \n",
    "                      #normalizedLandmark = handLandmarks.landmark[point]\n",
    "                      #pixelCoordinatesLandmark= drawingModule._normalized_to_pixel_coordinates(normalizedLandmark.x, normalizedLandmark.y, 640, 480)\n",
    "                      \n",
    "                      #Using the Finger Joint Identification Image we know that point 8 represents the tip of the Index Finger\n",
    "                      #if point == 8:\n",
    "                          #print(point)\n",
    "                          #print(pixelCoordinatesLandmark)\n",
    "                          #print(normalizedLandmark)\n",
    "            \n",
    "           #Below shows the current frame to the desktop \n",
    "           cv2.imshow(\"Frame\", frame1);\n",
    "           key = cv2.waitKey(1) & 0xFF\n",
    "           \n",
    "           #Below states that if the |q| is press on the keyboard it will stop the system\n",
    "           if key == ord(\"q\"):\n",
    "              break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
